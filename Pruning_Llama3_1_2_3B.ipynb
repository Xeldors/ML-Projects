{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11ed38b-26e2-48c4-b766-47ef4aea66a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7562abfe79fc4d07bd4b0b4fe99848f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir(\"C:/Users/sergi/Documents/Py/booksum\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from text_importing import read_text_file  # Ensure this function reads the file correctly\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "# Read the text file\n",
    "text = read_text_file(\"PrideandPrejudice/02.txt\")\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Encode the text to get tokens\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Get the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "\n",
    "def count_zero_weights(model):\n",
    "    zero_count = 0\n",
    "    total_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            total_count += module.weight.numel()  # Total number of weights\n",
    "            zero_count += (module.weight == 0).sum().item()  # Count of zero weights\n",
    "    return zero_count, total_count\n",
    "# Count zero weights in the original model\n",
    "zero_weights_original, total_weights_original = count_zero_weights(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5f7b1e-5552-45c1-b3ac-569a72da3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Pruning Linear Layers\n",
    "def prune_model(model, amount):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')  # Remove the pruning reparameterization to finalize it\n",
    "    return model\n",
    "\n",
    "LLAMA_pruned_V1=prune_model(model, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009ead21-9d6a-4051-a32a-da0092b2b590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/sergi/Documents/Py/booksum/pruned_20_LLAMA32_3B\\\\tokenizer_config.json',\n",
       " 'C:/Users/sergi/Documents/Py/booksum/pruned_20_LLAMA32_3B\\\\special_tokens_map.json',\n",
       " 'C:/Users/sergi/Documents/Py/booksum/pruned_20_LLAMA32_3B\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_model_path = \"C:/Users/sergi/Documents/Py/booksum/pruned_20_LLAMA32_3B\"\n",
    "LLAMA_pruned_V1.save_pretrained(pruned_model_path)\n",
    "tokenizer.save_pretrained(pruned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4640b024-a34d-4253-a2ab-67ca2add10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_weights(model):\n",
    "    zero_count = 0\n",
    "    total_count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            total_count += module.weight.numel()  # Total number of weights\n",
    "            zero_count += (module.weight == 0).sum().item()  # Count of zero weights\n",
    "    return zero_count, total_count\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c343a3-9e97-49a9-9e7b-5f3ccf2de018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero weights in pruned model: 642514994 out of 3212574720\n",
      "Zero weights in OG model: 0 out of 3212574720\n"
     ]
    }
   ],
   "source": [
    "# Count zero weights in the pruned model\n",
    "zero_weights_pruned, total_weights_pruned = count_zero_weights(LLAMA_pruned_V1)\n",
    "print(f\"Zero weights in pruned model: {zero_weights_pruned} out of {total_weights_pruned}\")\n",
    "print(f\"Zero weights in OG model: {zero_weights_original} out of {total_weights_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d611fb7-31fd-4a06-9a47-3a9344879701",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Importing a function that reads texts and the uploading the pride a prejudice 1st episode\n",
    "from text_importing import read_text_file  # Ensure this function reads the file correctly\n",
    "\n",
    "# Read the text file\n",
    "text = read_text_file(\"PrideandPrejudice/02.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132c3f10-8bba-4b30-8b54-3727b56cd83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Building the prompt that we will pass on the pruned and the og models\n",
    "prompt = f\"\"\"\n",
    "You are a text summarization expert with a talent for transforming complex information into clear and concise bullet points. You have extensive experience across various fields, which allows you to identify key events and relevant details accurately.\n",
    "Your task is to summarize the provided text into a list of bullet points. Focus only on the content given, emphasizing the most essential details. Make sure the summary is well-organized and highlights critical events or relevant information.\n",
    "Please return the summary as a list of not more than 7 bullet points, where each point captures an important event or piece of information from the text.\n",
    "The bullet points cannot exceed one line each.\n",
    "Here is the text to summarize:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05bc0129-e093-4624-97f7-156254d22f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5812759d3a34e6fb77a319afe916b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\sergi\\anaconda3\\envs\\sergi\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "##Testing OG: \n",
    "# Load the pruned model\n",
    "# Define the model ID\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Initialize the tokenizerconda\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# Encode the text to get tokens\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "# Get the number of tokens\n",
    "num_tokens = len(tokens)\n",
    "\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "####Prepare the summaries\n",
    "with torch.no_grad():\n",
    "    summary_OG = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=int(num_tokens),\n",
    "        return_full_text=False  # Ensuring that the output is not too short\n",
    "    )\n",
    "\n",
    "output_OG = summary_OG[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c62b29e-8fa2-478e-ab46-9ee33fac786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• It is a truth universally acknowledged that a single man with a good fortune must be in want of a wife.\n",
      "• Mrs. Bennet learns that Netherfield Park has been rented by a young man named Bingley from the north of England.\n",
      "• Bingley is a single man of large fortune, with an annual income of four to five thousand pounds.\n",
      "• Mrs. Bennet believes that Bingley may fall in love with one of her daughters and encourages her husband to visit him.\n",
      "• Mr. Bennet is skeptical and thinks that visiting Bingley will be a waste of time, but ultimately agrees to go.\n",
      "• The Bennet family is eager to meet Bingley and secure a marriage for their daughters.\n",
      "• Mrs. Bennet's primary goal is to get her daughters married, and she is willing to do whatever it takes to achieve this goal.\n"
     ]
    }
   ],
   "source": [
    "output_OG=output_OG.split(\"bullet points:\")[-1].strip()\n",
    "print(output_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8f235c-adcb-4129-bf2f-694ec909af0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d752f31afb6425b9e1264f2d295cd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "##Testing pruned: \n",
    "\n",
    "# Load the pruned model\n",
    "pruned_model = AutoModelForCausalLM.from_pretrained(pruned_model_path, torch_dtype=torch.bfloat16)\n",
    "pruned_tokenizer = AutoTokenizer.from_pretrained(pruned_model_path)\n",
    "\n",
    "# Initialize the pipeline with the pruned model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=pruned_model,\n",
    "    tokenizer=pruned_tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Prepare the summaries with no gradient tracking\n",
    "with torch.no_grad():\n",
    "    summary_pruned = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=int(num_tokens),\n",
    "        return_full_text=False  # Ensuring that the output is not too short\n",
    "    )\n",
    "\n",
    "# Extract the generated text\n",
    "output_pruned = summary_pruned[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5013c403-230a-49d2-8b9d-778a8c315ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• The arrival of a single man with a good fortune in the neighborhood sets off a chain of events related to marriage and social standing.\n",
      "• Mrs. Bennet is eager to inform her husband about the new resident, Netherfield Park, which is being occupied by a young man named Bingley.\n",
      "• Bingley is a single man of large fortune, with a yearly income of four or five thousand pounds, which sparks interest in the Bennet family's daughters for marriage.\n",
      "• The Bennet family is urged to visit Bingley to facilitate potential marriages between their daughters and him.\n",
      "• Mr. Bennet is reluctant to visit Bingley, but his wife persists, suggesting that he should do so to secure advantageous marriages for their daughters.\n",
      "• The Bennet family's dynamics and personalities are revealed through their interactions with each other and with their daughters.\n",
      "• The main character, Mrs. Bennet, is portrayed as a woman driven by her desire to marry off her daughters and secure their social standing, while her husband is depicted as a more reserved and sarcastic individual.\n"
     ]
    }
   ],
   "source": [
    "output_pruned=output_pruned.split(\":\")[-1].strip()\n",
    "print(output_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29fe0958-e0fe-43f4-ba0b-95f2b8f888e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 2335.75732421875\n",
      "Cosine Similarity: 0.9668498635292053\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the outputs to get input IDs\n",
    "\n",
    "tokens_OG = tokenizer(output_OG, return_tensors='pt')\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokens_pruned = pruned_tokenizer(output_pruned, return_tensors='pt').to(device)\n",
    "pruned_model=pruned_model.to(device)\n",
    "\n",
    "# Tokenize inputs and move tensors to CUDA\n",
    "# tokens_OG = tokenizer(output_OG, return_tensors='pt').to(device)\n",
    "# tokens_pruned = pruned_tokenizer(output_pruned, return_tensors='pt').to(device)\n",
    "\n",
    "# Obtain embeddings from the models\n",
    "with torch.no_grad():\n",
    "    # For the original model\n",
    "    outputs_OG = model(**tokens_OG, output_hidden_states=True)\n",
    "    embeddings_OG = outputs_OG.hidden_states[-1].mean(dim=1)  # Get last layer and mean pooling\n",
    "\n",
    "    # For the pruned model\n",
    "    outputs_pruned = pruned_model(**tokens_pruned, output_hidden_states=True)\n",
    "    embeddings_pruned = outputs_pruned.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "# Convert embeddings to float32 to ensure compatibility with dot product operations\n",
    "embeddings_OG = embeddings_OG.float()\n",
    "embeddings_pruned = embeddings_pruned.float()\n",
    "\n",
    "# Compute the dot product of the two embeddings\n",
    "dot_product = torch.dot(embeddings_OG.squeeze(), embeddings_pruned.squeeze())\n",
    "print(f\"Dot Product: {dot_product.item()}\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "# Normalize the embeddings\n",
    "norm_embeddings_OG = torch.norm(embeddings_OG)\n",
    "norm_embeddings_pruned = torch.norm(embeddings_pruned)\n",
    "\n",
    "# Cosine similarity calculation\n",
    "cosine_similarity = dot_product / (norm_embeddings_OG * norm_embeddings_pruned)\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
