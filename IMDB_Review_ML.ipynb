{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cd0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "728ccd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##extracting data\n",
    "# # Path to the .tar.gz file\n",
    "# tar_file_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb_v1.tar.gz\"\n",
    "# extract_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb\"\n",
    "\n",
    "# # Extract the tar.gz file\n",
    "# with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8176eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading text \n",
    "def read_text_files(directory):\n",
    "    text_data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".txt\"):  # Ensure we are reading only text files\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    text_data.append(content)\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee01af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_reviews_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb\\aclImdb\\train\\pos\"\n",
    "train_neg_reviews_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb\\aclImdb\\train\\neg\"\n",
    "test_pos_reviews_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb\\aclImdb\\test\\pos\"\n",
    "test_neg_reviews_path = r\"C:\\Users\\sergi\\Documents\\Py\\IMDB_ML\\aclImdb\\aclImdb\\test\\neg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c3fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Positive=(read_text_files(train_pos_reviews_path))\n",
    "Test_Positive=(read_text_files(test_pos_reviews_path))\n",
    "Train_Negative=(read_text_files(train_neg_reviews_path))\n",
    "Test_Negative=(read_text_files(test_neg_reviews_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d81311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 2\n",
    "\n",
    "# Generate labels (one-hot encoded)\n",
    "Train_pos = to_one_hot([0] * len(Train_Positive), num_classes)\n",
    "Train_neg = to_one_hot([1] * len(Train_Negative), num_classes)\n",
    "Test_pos = to_one_hot([0] * len(Test_Positive), num_classes)\n",
    "Test_neg = to_one_hot([1] * len(Test_Negative), num_classes)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "split_index_train = int(len(Train_Positive) / 2)\n",
    "split_index_val = int(len(Test_Positive) / 2)\n",
    "\n",
    "# Training data\n",
    "in_Train_positive = Train_Positive[:split_index_train]\n",
    "in_Train_negative = Train_Negative[:split_index_train]\n",
    "Train_txt = in_Train_positive + in_Train_negative\n",
    "Train_out = np.concatenate((Train_pos[:split_index_train], Train_neg[:split_index_train]))\n",
    "\n",
    "# Validation data\n",
    "in_val_positive = Test_Positive[:split_index_val]\n",
    "in_val_negative = Test_Negative[:split_index_val]\n",
    "Vali_txt = in_val_positive + in_val_negative\n",
    "Vali_out = np.concatenate((Test_pos[:split_index_val], Test_neg[:split_index_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341db2cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##preprocessing using textvectorisation\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    output_mode=\"tf_idf\",\n",
    "    max_tokens=1000  # Reduce based on your memory capacity\n",
    ")\n",
    "# Adapting the layer to the training data\n",
    "text_vec_layer.adapt(Train_txt)\n",
    "\n",
    "# Vectorizing the data\n",
    "Vectorised_Train = text_vec_layer(Train_txt)\n",
    "Vectorised_Val = text_vec_layer(Vali_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8a4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5000, activation='relu'),  \n",
    "    tf.keras.layers.Dense(1000, activation='relu'),  \n",
    "    tf.keras.layers.Dense(500, activation='relu'),  \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Output layer with softmax activation with softmax\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac0e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',  # Use binary_crossentropy because we are dealing with either good or bad classification\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db966a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((Vectorised_Train, Train_out)).batch(1000)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((Vectorised_Val, Vali_out)).batch(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd9ef68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 353ms/step - accuracy: 0.5645 - loss: 81.3945 - val_accuracy: 0.5000 - val_loss: 9.5735\n",
      "Epoch 2/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 290ms/step - accuracy: 0.0845 - loss: 11.1096 - val_accuracy: 0.5207 - val_loss: 0.7322\n",
      "Epoch 3/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 259ms/step - accuracy: 0.2963 - loss: 0.9901 - val_accuracy: 0.5000 - val_loss: 0.7198\n",
      "Epoch 4/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 254ms/step - accuracy: 0.1108 - loss: 0.8670 - val_accuracy: 0.5914 - val_loss: 0.6752\n",
      "Epoch 5/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 283ms/step - accuracy: 0.2850 - loss: 0.7262 - val_accuracy: 0.5026 - val_loss: 0.6861\n",
      "Epoch 6/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 284ms/step - accuracy: 0.1450 - loss: 0.8161 - val_accuracy: 0.5840 - val_loss: 0.6546\n",
      "Epoch 7/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 306ms/step - accuracy: 0.3277 - loss: 0.7538 - val_accuracy: 0.5134 - val_loss: 0.6967\n",
      "Epoch 8/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 287ms/step - accuracy: 0.1898 - loss: 0.8652 - val_accuracy: 0.6080 - val_loss: 0.6176\n",
      "Epoch 9/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 286ms/step - accuracy: 0.3940 - loss: 0.7058 - val_accuracy: 0.5456 - val_loss: 0.8425\n",
      "Epoch 10/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - accuracy: 0.3348 - loss: 0.9858 - val_accuracy: 0.5872 - val_loss: 0.6074\n",
      "Epoch 11/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 281ms/step - accuracy: 0.3346 - loss: 0.7320 - val_accuracy: 0.6570 - val_loss: 0.5903\n",
      "Epoch 12/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 267ms/step - accuracy: 0.5466 - loss: 0.7350 - val_accuracy: 0.6140 - val_loss: 0.6568\n",
      "Epoch 13/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 304ms/step - accuracy: 0.4207 - loss: 0.8710 - val_accuracy: 0.8061 - val_loss: 0.5017\n",
      "Epoch 14/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 320ms/step - accuracy: 0.8089 - loss: 0.5108 - val_accuracy: 0.7052 - val_loss: 0.6354\n",
      "Epoch 15/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 301ms/step - accuracy: 0.5836 - loss: 0.7448 - val_accuracy: 0.8430 - val_loss: 0.4324\n",
      "Epoch 16/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - accuracy: 0.8045 - loss: 0.4664 - val_accuracy: 0.7570 - val_loss: 0.5259\n",
      "Epoch 17/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 309ms/step - accuracy: 0.6785 - loss: 0.6197 - val_accuracy: 0.8206 - val_loss: 0.4302\n",
      "Epoch 18/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 301ms/step - accuracy: 0.7711 - loss: 0.4954 - val_accuracy: 0.8279 - val_loss: 0.4279\n",
      "Epoch 19/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 299ms/step - accuracy: 0.7946 - loss: 0.4532 - val_accuracy: 0.8164 - val_loss: 0.4705\n",
      "Epoch 20/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - accuracy: 0.7666 - loss: 0.4975 - val_accuracy: 0.8466 - val_loss: 0.4000\n",
      "Epoch 21/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 299ms/step - accuracy: 0.8296 - loss: 0.3840 - val_accuracy: 0.8202 - val_loss: 0.5365\n",
      "Epoch 22/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 294ms/step - accuracy: 0.7921 - loss: 0.4649 - val_accuracy: 0.8379 - val_loss: 0.4405\n",
      "Epoch 23/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 308ms/step - accuracy: 0.8228 - loss: 0.3894 - val_accuracy: 0.8253 - val_loss: 0.5653\n",
      "Epoch 24/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 305ms/step - accuracy: 0.8133 - loss: 0.4151 - val_accuracy: 0.8485 - val_loss: 0.4220\n",
      "Epoch 25/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 302ms/step - accuracy: 0.8458 - loss: 0.3420 - val_accuracy: 0.8410 - val_loss: 0.4593\n",
      "Epoch 26/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 307ms/step - accuracy: 0.8298 - loss: 0.3710 - val_accuracy: 0.8516 - val_loss: 0.4589\n",
      "Epoch 27/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 301ms/step - accuracy: 0.8610 - loss: 0.3144 - val_accuracy: 0.8163 - val_loss: 0.6657\n",
      "Epoch 28/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 291ms/step - accuracy: 0.7932 - loss: 0.4121 - val_accuracy: 0.8426 - val_loss: 0.5263\n",
      "Epoch 29/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 289ms/step - accuracy: 0.8472 - loss: 0.3337 - val_accuracy: 0.8520 - val_loss: 0.5371\n",
      "Epoch 30/30\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 302ms/step - accuracy: 0.8648 - loss: 0.2986 - val_accuracy: 0.8524 - val_loss: 0.5281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17d428f94d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=30, validation_data=val_dataset)  # Validation data is optional but useful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1612e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"C:\\\\Users\\\\sergi\\\\Documents\\\\Py\\\\IMDB_ML\\\\IMBD_reviews_vectorisation_v1.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be37ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8408 - loss: 0.4694\n",
      "Categorical Cross entropy Loss: 0.5280663967132568\n",
      "Test Accuracy: 0.852400004863739\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(val_dataset)\n",
    "print(f'Categorical Cross entropy Loss: {test_loss}') ###Loss value on th last dataset\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49dc316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transforming the softmax output into final output\n",
    "def pos_net(pre):\n",
    "    a=(pre.tolist())\n",
    "    a=[round(i) for i in a]\n",
    "    if round(a[0])>0.5: \n",
    "        return \"pos\"\n",
    "    if round(a[1])>0.5: \n",
    "        return \"neg\"\n",
    "    else:\n",
    "        return \"Not properly classified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48a0a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "##Making prediction on only negative reviews\n",
    "Vectorised_Test_neg = text_vec_layer(Test_Negative)\n",
    "Predictions_Negative = model.predict(Vectorised_Test_neg)\n",
    "\n",
    "Predictions_Negative_Output=[pos_net(Predictions_Negative[i]) for i in range(0,len(Predictions_Negative))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2850d1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for negative testing set: 0.87\n"
     ]
    }
   ],
   "source": [
    "count_neg = Predictions_Negative_Output.count(\"neg\")\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions_neg_set = len(Predictions_Negative_Output)\n",
    "\n",
    "# Calculate the proportion\n",
    "accuracy_neg = count_neg / total_predictions_neg_set\n",
    "\n",
    "# Print the result using formatted strings\n",
    "print(f\"Accuracy for negative testing set: {accuracy_neg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91db2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "##Making prediction on only positive reviews\n",
    "Vectorised_Test_pos = text_vec_layer(Test_Positive)\n",
    "Predictions_Positive = model.predict(Vectorised_Test_pos)\n",
    "\n",
    "Predictions_Positive_Output=[pos_net(Predictions_Positive[i]) for i in range(0,len(Predictions_Positive))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bce5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for positive testing set: 0.83\n"
     ]
    }
   ],
   "source": [
    "count_pos = Predictions_Positive_Output.count(\"pos\")\n",
    "\n",
    "# Calculate the total number of predictions\n",
    "total_predictions_pos_set = len(Predictions_Positive_Output)\n",
    "\n",
    "# Calculate the proportion\n",
    "accuracy_pos = count_pos / total_predictions_pos_set\n",
    "\n",
    "# Print the result using formatted strings\n",
    "print(f\"Accuracy for positive testing set: {accuracy_pos:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb546ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for overall testing set: 0.85\n"
     ]
    }
   ],
   "source": [
    "##Total testing set accuracy\n",
    "Testing_Accuracy=(count_pos+count_neg)/(total_predictions_neg_set+total_predictions_pos_set)\n",
    "print(f\"Accuracy for overall testing set: {Testing_Accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2c143",
   "metadata": {},
   "source": [
    "##performed on the data obtained in; \n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
