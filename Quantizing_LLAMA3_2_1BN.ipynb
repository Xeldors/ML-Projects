{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0661b14d-9533-41bf-aa3e-4477013f94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b259ce-f5ec-4e9f-b39c-3fef4bc654fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\sergi\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantized_path = \"C:/Users/sergi/Documents/Py/booksum/quantized_LLAMA1B\"\n",
    "\n",
    "login(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e67a91-7cf2-4f02-8bdf-84ecd078b47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/sergi/Documents/Py/booksum/quantized_LLAMA1B\\\\tokenizer_config.json',\n",
       " 'C:/Users/sergi/Documents/Py/booksum/quantized_LLAMA1B\\\\special_tokens_map.json',\n",
       " 'C:/Users/sergi/Documents/Py/booksum/quantized_LLAMA1B\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quantizing to 8bts\n",
    "\n",
    "# Define the quantization configuration for 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Use 8-bit quantization\n",
    ")\n",
    "\n",
    "# Load the quantized model with the configuration\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,  \n",
    "    quantization_config=quantization_config,  # Correctly pass quantization config\n",
    "    device_map=\"cuda\"  # Ensure it loads to GPU\n",
    ")\n",
    "\n",
    "# Save the quantized model and tokenizer\n",
    "model_8bit.save_pretrained(quantized_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(quantized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d69b1dc-6ff2-414a-a7d3-e89f762a50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens output, Temperature, and sampling top p parameters\n",
    "top_p = 0.3\n",
    "temperature = 0.3\n",
    "max_new_tokens = 500\n",
    "\n",
    "# Define the prompt text\n",
    "prompt_text = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a wise historian specialized in the Roman Empire. \"\n",
    "                   \"You convey information about the Roman Empire in a condensed form of bullet points.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\" Please explain to me the downfall of the Roman Republic. \"\n",
    "                   f\"Narrow the explanation so that you only utilize this amount of tokens {max_new_tokens}.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc0770-d5aa-412b-a91f-3bbc2fd26cff",
   "metadata": {},
   "source": [
    "-Testing the Quantized model vs the Non Quantized. -\n",
    "\n",
    "To evaluate the testing two key metrics are considered:\n",
    "1) Cosine similarity: Embeddings of the outputs dot product\n",
    "2) Inference time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df1e09ea-a95b-4d0f-9f5b-fc5c4e8405cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a concise explanation of the downfall of the Roman Republic:\n",
      "\n",
      "**Causes of the Downfall:**\n",
      "\n",
      "* **Economic troubles:** Heavy debt, inflation, and a decline in trade led to economic instability.\n",
      "* **Military overextension:** The Roman Republic's extensive military campaigns drained resources and led to a decline in the legions' effectiveness.\n",
      "* **Corruption and mismanagement:** The increasing power of the Senate and the equestrian class led to corruption and abuse of power.\n",
      "* **External pressures:** The rise of the Punic Wars and the threat of external invasions weakened the Republic's defenses.\n",
      "* **Social and cultural changes:** The growing influence of the equestrian class and the decline of traditional Roman values contributed to social and cultural changes.\n",
      "\n",
      "**Key Events:**\n",
      "\n",
      "* **The Second Punic War (218-201 BCE):** Hannibal's invasion of Italy led to a decline in the Republic's military power and a shift in the balance of power.\n",
      "* **The Gracchan Reforms (133-121 BCE):** The Gracchi brothers' attempts to address economic and social issues led to increased corruption and instability.\n",
      "* **The Optimates and Populares:** The rise of the Optimates (conservative faction) and Populares (liberal faction) led to a decline in the Republic's stability and a shift towards authoritarianism.\n",
      "\n",
      "**Consequences:**\n",
      "\n",
      "* **The End of the Roman Republic:** The Republic's decline led to the rise of the Roman Empire under Augustus Caesar in 27 BCE.\n"
     ]
    }
   ],
   "source": [
    "#Testing the quantized vs the non quantized\n",
    "#Non-Quantized\n",
    "import time\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "\n",
    "start_time_og = time.time()  # Start timer \n",
    "\n",
    "\n",
    "# Initialize pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",                                                                                                               \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Generate output with the model in inference mode\n",
    "with torch.inference_mode():\n",
    "    outputs = pipe(\n",
    "        prompt_text,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        return_full_text=False\n",
    "    )\n",
    "\n",
    "# Extract the generated text\n",
    "OGMODEL = outputs[0][\"generated_text\"]\n",
    "\n",
    "# End timer and calculate inference time\n",
    "end_time_quant = time.time()\n",
    "inference_time_OG = end_time_quant - start_time_og  \n",
    "\n",
    "print(OGMODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31c25723-506f-4225-b2a6-dcfbc68f13b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a concise explanation of the downfall of the Roman Republic:\n",
      "\n",
      "**Causes of the Downfall:**\n",
      "\n",
      "* **Overextension and Military Overreach**: Rome's extensive military campaigns and conquests drained its resources, leading to economic strain and social unrest.\n",
      "* **Economic Crisis**: The Roman economy suffered from inflation, debasement of the currency, and a decline in trade, making it difficult for the government to fund its military and administrative costs.\n",
      "* **Social Inequality and Corruption**: The Roman Republic's social hierarchy became increasingly unequal, with the wealthy elite holding power and influence, while the poor and middle class suffered.\n",
      "* **Lack of Effective Leadership**: The Roman Republic's leadership was plagued by infighting, power struggles, and ineffective decision-making, leading to a breakdown in governance.\n",
      "* **External Pressures**: The Roman Republic faced external threats from neighboring states, such as the Gauls and the Parthians, which further strained its resources and weakened its position.\n",
      "\n",
      "**Key Events:**\n",
      "\n",
      "* **The Social War (91-86 BCE)**: A civil war that weakened the Roman Republic and led to the rise of the Optimates and Populares factions.\n",
      "* **The Gracchan Reforms (133-121 BCE)**: A series of land reforms that aimed to reduce inequality and promote social justice, but ultimately failed to address the underlying issues.\n",
      "* **The Crisis of the Late Republic (100-44 BCE)**: A period of economic and social crisis that weakened the Roman Republic and paved the way for the rise of the Roman Empire.\n",
      "\n",
      "**Consequences:**\n",
      "\n",
      "* **The End of the Roman Republic**: The Roman Republic's decline and eventual fall marked the end of a centuries-long period of Roman dominance and the beginning of the Roman Empire.\n"
     ]
    }
   ],
   "source": [
    "#Quantized\n",
    "# Quantized model loading\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(quantized_path, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_path)\n",
    "\n",
    "start_time_quant = time.time()  # Start timer \n",
    "\n",
    "# Create a pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",                                                                                                               \n",
    "    model=model_8bit,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Generate text with inference mode (no gradient tracking)\n",
    "with torch.inference_mode():\n",
    "    outputs = pipe(prompt_text,\n",
    "                do_sample=True,\n",
    "                top_p=top_p,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                return_full_text=False)\n",
    "\n",
    "QUANTMODEL = outputs[0][\"generated_text\"]\n",
    "\n",
    "end_time_quant = time.time()  \n",
    "inference_time_Quant = end_time_quant - start_time_quant  \n",
    "\n",
    "print(QUANTMODEL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e4149d-9d9e-49e3-9eca-507bcee08d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 2086.2724609375\n",
      "Cosine Similarity: 0.972548246383667\n"
     ]
    }
   ],
   "source": [
    "# Ensure that device is set to GPU if available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Tokenize the outputs to get input IDs (move tokenized input to the device)\n",
    "tokens_OG = tokenizer(OGMODEL, return_tensors='pt').to(device)\n",
    "tokens_quant = tokenizer(QUANTMODEL, return_tensors='pt').to(device)\n",
    "\n",
    "# Obtain embeddings from the models\n",
    "with torch.no_grad():\n",
    "    # For the original model\n",
    "    outputs_OG = model(**tokens_OG, output_hidden_states=True)\n",
    "    embeddings_OG = outputs_OG.hidden_states[-1].mean(dim=1)  # Get last layer and mean pooling\n",
    "\n",
    "    # For the pruned/quantized model\n",
    "    outputs_quant = model_8bit(**tokens_quant, output_hidden_states=True)\n",
    "    embeddings_quant = outputs_quant.hidden_states[-1].mean(dim=1)\n",
    "\n",
    "# Convert embeddings to float32 to ensure compatibility with dot product operations\n",
    "embeddings_OG = embeddings_OG.float()\n",
    "embeddings_quant = embeddings_quant.float()\n",
    "\n",
    "# Compute the dot product of the two embeddings\n",
    "dot_product = torch.dot(embeddings_OG.squeeze(), embeddings_quant.squeeze())\n",
    "print(f\"Dot Product: {dot_product.item()}\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "# Normalize the embeddings\n",
    "norm_embeddings_OG = torch.norm(embeddings_OG)\n",
    "norm_embeddings_quant = torch.norm(embeddings_quant)\n",
    "\n",
    "# Cosine similarity calculation\n",
    "cosine_similarity = dot_product / (norm_embeddings_OG * norm_embeddings_quant)\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51549a7-23f5-412b-95a0-0157dc830162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Original 1B LLAMA took: 101.02750778198242 seconds or 1:41 minutes\n",
      "The Quantized to 8BTS 1B LLAMA took: 41.79430270195007 seconds or 0:42 minutes\n"
     ]
    }
   ],
   "source": [
    "#Checking the inference time\n",
    "def seconds_to_minutes_seconds(seconds):\n",
    "    minutes = int(seconds // 60) \n",
    "    remaining_seconds = seconds % 60 \n",
    "    return (f\"{minutes}:{int(round(remaining_seconds,0))}\")\n",
    "\n",
    "\n",
    "print(f\"The Original 1B LLAMA took: {inference_time_OG} seconds or {seconds_to_minutes_seconds(inference_time_OG)} minutes\")\n",
    "print(f\"The Quantized to 8BTS 1B LLAMA took: {inference_time_Quant} seconds or {seconds_to_minutes_seconds(inference_time_Quant)} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
